{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - _Foundations of Information Retrieval 2023_\n",
    "\n",
    "This assignment is divided in 4 parts, which have to be delivered all together no later than 01/10/2023 at 23:59 (strict - no extensions will be granted!) via Canvas. Delivery of the assignment solutions is mandatory (_see grading conditions on Canvas and in slides of Lecture01_).\n",
    "\n",
    "We will use [ElasticSearch](https://www.elastic.co/) as search engine. It provides state-of-the-art tools to implement your own engine, index your documents, and let you focus on methodological aspects of search models and optimization. \n",
    "\n",
    "The assignment is about text-based Information Retrieval and it is structured in three parts:\n",
    "1. IR performance evaluation (implementation of performance metrics)\n",
    "2. Setting up a search engine, pre-processing and indexing using ElasticSearch (Indexing, Analyzers)\n",
    "3. Implementation and optimization of models of search (Similarity)\n",
    "\n",
    "\n",
    "This assignment contains exercises, marked with the section title __Exercise 01.(x)__, which are evaluated, and other sections that contain support code which you should study and use as it is. Write your answers between the comments `BEGIN ANSWER` and `END ANSWER`. \n",
    "\n",
    "_Note:_ the comment `#THIS IS GRADED!` in a section indicates that it will be graded.\n",
    "\n",
    "\n",
    "### Initial preparation (self-study)\n",
    "For the first part, it is good to acquire (or refresh) basic knowledge of Python. Please use the [Python tutorials](https://docs.python.org/3/tutorial/) if needed.\n",
    "\n",
    "For the second and third part of the assignment, please study yourself the [Getting Started guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html) of ElasticSearch and get acquainted with the framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "# PART 01 - Performance evaluation\n",
    "\n",
    "\n",
    "### Background information and reading\n",
    "Study the slides of Lecture 01 (available on Canvas) and the reference book chapter (Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, [Chapter 8, Evaluation in information retrieval](http://nlp.stanford.edu/IR-book/pdf/08eval.pdf), Cambridge University Press. 2008)\n",
    "\n",
    "### Basic concepts\n",
    "Suppose the set of relevant documents (the document identifiers - _doc-IDs_) is called `relevant`, then we  define it as follows (in Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = set([2, 3, 5, 8, 13, 17, 21, 34, 38])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect run would retrieve exactly these 9 documents in any order. Now, suppose the list of retrieved documents (the document identifiers - _doc-IDs_) is called `retrieved`, and contains the following _doc-IDs_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = [14, 4, 2, 18, 16, 8, 46, 32, 17, 34, 33, 22, 47, 39, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest evaluation measures is the _Success at rank 1_, i.e. `Is the first document retrieved a relevant document?`\n",
    "\n",
    "_Success at rank 1_ returns 1 if the first document is relevant, and 0 otherwise. A possible implementation is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def success_at_1 (relevant, retrieved):\n",
    "    if len(retrieved) > 0 and retrieved[0] in relevant:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "success_at_1(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first retrieved documentid is 14 which is not in the set of relevant documents, so the `success_at_1` is 0.\n",
    "\n",
    "_________________\n",
    "\n",
    "> Note how easy it is to check if an item occurs in a Python set or list by using the keyword: `in`. Similarly, you can loop over all items in a set of list with: \n",
    "`for doc in retrieved:`, \n",
    "where doc will refer to each item in the set or list. \n",
    "\n",
    "Be sure to use the internet to sharpen your knowledge about Python constructs, for instance on [Python list slicing](https://duckduckgo.com/?q=python+list+slicing). Also note that the code above checks if at least one document is retrieved to avoid an index out of bounds exception (i.e. we avoid to access an empty vector).\n",
    "\n",
    "> ___Suggestion:___ _to be sure of the correctness of the implementation of the performance metrics, you can compute their values manually and compare them with those computed by your functions. This is important, as you will use these metrics for later exercises and to compare the results of differentmodels._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation exercise: _Success at k_\n",
    "The measure _Success at k_ returns 1 if a relevant document is among the first _k_ documents retrieved and zero otherwise.\n",
    "\n",
    "> Success at _k_ measures are well-suited in case there is typically only one relevant document (or retrieving one relevant document is enough).\n",
    "\n",
    " __Implement _Success at 5_ below.__ \n",
    " > The correct result is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def success_at_k(relevant, retrieved, k):\n",
    "    if len(retrieved) >= k:\n",
    "        for element in retrieved[:k]:\n",
    "            if element in relevant:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def success_at_5(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    return success_at_k(relevant, retrieved, k=5)\n",
    "    # END ANSWER\n",
    "    \n",
    "success_at_5(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly __implement success at rank 10__\n",
    "\n",
    "> The correct result is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def success_at_10(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    return success_at_k(relevant, retrieved, k=10)\n",
    "    # END ANSWER\n",
    "    \n",
    "success_at_10(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.A: _Precision, Recall and F-measure_\n",
    "__1. Implement _Precision_ using Formula 8.1 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).__\n",
    "\n",
    ">_Hint:_ one can count the number of documents in a list using the built-in Python function [len()](https://docs.python.org/3/library/functions.html#len) \\\n",
    "> _example:_ `len(retrieved)` for the number of retrieved documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def relevant_items_retrieved(relevant, retrieved):\n",
    "    return sum([retrieved_doc in relevant for retrieved_doc in retrieved])\n",
    "\n",
    "def precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    return relevant_items_retrieved(relevant, retrieved) / len(retrieved)\n",
    "    # END ANSWER\n",
    "    \n",
    "precision(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Implement _Recall_ using Formula 8.2 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def recall(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    return relevant_items_retrieved(relevant, retrieved) / len(relevant)\n",
    "    # END ANSWER\n",
    "    \n",
    "recall(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Implement the balanced F measure (_F_ with β=1) using Formula 8.6 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).__\n",
    "\n",
    "> Tip: you may reuse your implementations of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def f_measure(relevant, retrieved, beta=1):\n",
    "    # BEGIN ANSWER\n",
    "    return (beta*beta + 1) * recall(relevant, retrieved) * precision(relevant, retrieved) / ((beta*beta) * precision(relevant, retrieved) + recall(relevant, retrieved))\n",
    "    # END ANSWER\n",
    "    \n",
    "f_measure(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.B: _Precision at rank k_ and  _R-Precision_\n",
    "\n",
    "Precision, Recall and F are _set_-based measures and suited for unranked lists of documents. If our search system returns a ranked _list_ of results, we can measure precision for several cut-off levels _k_ in the ranked list, i.e. we evaluate the relevance of the TOP-_k_ retrieved documents _(see lecture 01 slides and the book chapter)_. \n",
    "\n",
    "\n",
    "**1. Implement the function `precision_at_k()` that measures the precision at rank _k_**\n",
    "\n",
    "> Interesting fact: For _k_=1, the _Precision at rank 1_ would be the samen as _Success at rank 1_ (why?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr@1: 0.00\n",
      "Pr@5: 0.20\n",
      "Pr@10: 0.40\n"
     ]
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    # BEGIN ANSWER\n",
    "    return precision(relevant, retrieved[:k])\n",
    "    # END ANSWER\n",
    "    \n",
    "\n",
    "\n",
    "print('Pr@1: %1.2f' % precision_at_k(relevant, retrieved, k=1))\n",
    "print('Pr@5: %1.2f' % precision_at_k(relevant, retrieved, k=5))\n",
    "print('Pr@10: %1.2f' % precision_at_k(relevant, retrieved, k=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Implement R-Precision as defined in Chapter 8 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def r_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    return precision_at_k(relevant, retrieved, len(relevant))\n",
    "    # END ANSWER\n",
    "    \n",
    "r_precision(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.D:  Interpolated precision at _recall_ X\n",
    "\n",
    "Another way to address ranked retrieval is to measure precision for several _recall_ levels _X_.\n",
    "\n",
    "__Implement the function `interpolated_precision_at_recall_X()` that measures the interpolated precision at recall level _X_ as defined by formula 8.7 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).__\n",
    "\n",
    "> Tip: calculate for each rank the recall. If the recall is greater than or equal to X, \n",
    "> calculate the precision. Keep the highest (maximum) precision of those to be returned at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr_i@Re01: 0.40\n",
      "Pr_i@Re02: 0.40\n",
      "Pr_i@Re03: 0.40\n",
      "Pr_i@Re04: 0.40\n",
      "Pr_i@Re05: 0.00\n",
      "Pr_i@Re06: 0.00\n",
      "Pr_i@Re07: 0.00\n",
      "Pr_i@Re08: 0.00\n",
      "Pr_i@Re09: 0.00\n",
      "Pr_i@Re10: 0.00\n"
     ]
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "def recall_at_k(relevant, retrieved, k):\n",
    "    return recall(relevant, retrieved[:k])\n",
    "\n",
    "#for i in range(len(retrieved)):\n",
    "    #print(precision_at_k(relevant, retrieved, i + 1), recall_at_k(relevant, retrieved, i + 1))\n",
    "\n",
    "def interpolated_precision_at_recall_X (relevant, retrieved, X):\n",
    "    # BEGIN ANSWER\n",
    "    recalls_at_ranks = {}\n",
    "    max_prec = 0.0\n",
    "    for k in range(len(retrieved)):\n",
    "        re_at_k = recall_at_k(relevant, retrieved, k=k+1)\n",
    "        if re_at_k >= X:\n",
    "            #print(re_at_k)\n",
    "            prec_at_k = precision_at_k(relevant, retrieved, k=k+1)\n",
    "            if prec_at_k > max_prec:\n",
    "                max_prec = prec_at_k\n",
    "    return max_prec\n",
    "    # END ANSWER\n",
    "    \n",
    " \n",
    "\n",
    "print('Pr_i@Re01: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.1))\n",
    "print('Pr_i@Re02: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.2))\n",
    "print('Pr_i@Re03: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.3))\n",
    "print('Pr_i@Re04: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.4))\n",
    "print('Pr_i@Re05: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.5))\n",
    "print('Pr_i@Re06: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.6))\n",
    "print('Pr_i@Re07: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.7))\n",
    "print('Pr_i@Re08: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.8))\n",
    "print('Pr_i@Re09: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=0.9))\n",
    "print('Pr_i@Re10: %1.2f' % interpolated_precision_at_recall_X(relevant, retrieved, X=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.E:  _Average Precision_\n",
    "\n",
    "For a single information need, _Average Precision_ is the average of the precision value obtained for the set of top k documents existing after each relevant document is retrieved (see [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book), Pages 159 and 160). \n",
    "\n",
    "__Implement _Average Precision_ for a single information need.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15555555555555556"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    precisions = [0 for i in range(len(relevant))]\n",
    "    relevant_found = 0\n",
    "    for i in range(len(retrieved)):\n",
    "        if retrieved[i] in relevant:\n",
    "            precisions[relevant_found] = precision_at_k(relevant, retrieved, k=i+1)\n",
    "            relevant_found += 1\n",
    "    return sum(precisions)/len(precisions)\n",
    "            \n",
    "    # END ANSWER\n",
    "\n",
    "average_precision(relevant, retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Performance measures in TREC benchmarks\n",
    "\n",
    "The relevance judgments are provided by TREC in so-called _\"qrels\"_ files that look as follows:\n",
    "\n",
    "    1000 Q0 1341 1\n",
    "    1000 Q0 1231 0\n",
    "    1001 Q0 12332 1\n",
    "     ...\n",
    "\n",
    "The columns of the _qrels_ file contain:\n",
    "1. the query identifier\n",
    "2. the query number within that topic (currently unused and should always be Q0)\n",
    "3. the document identifier that was examined by the judges\n",
    "4. the relevance of the document (_1_:relevant; _0_: not relevant).\n",
    "\n",
    "Below we provide some Python code that reads the _qrels_ and the _run_. The qrels will be put in the Python dictionary `all_relevant`. A [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) provides quick lookup of a set of values given a key. We will use the `query_id` as a key, and a [Python set](https://docs.python.org/3/tutorial/datastructures.html#sets) of relevant document identifiers. For the partial qrels file above, `all_relevant` would look as follows:\n",
    "\n",
    "    {\n",
    "        \"1000\": set([\"1341\", \"1231\"]),\n",
    "        \"1001\": set([\"12332\"])\n",
    "    }\n",
    "    \n",
    "We will use a dictionary called `all_retrieved` with `query_id` as key, and as value a [Python list](https://docs.python.org/3/tutorial/introduction.html#lists) of document identifiers retrieved by the IR system:\n",
    "\n",
    "    {\n",
    "        \"1000\": [\"1341\", \"12346, \"2345\"],\n",
    "        \"1001\": [..., ..., ...],\n",
    "        ...\n",
    "    }\n",
    "\n",
    "Note that, with this data structure, for each `query_id` we can easily access the list of retrieved and relevant documents, and compute the performance metrics. We can then average these measures over all the queries to compute the mean performance of the IR system on the given retrieval task.\n",
    "\n",
    "Please examine the code below, and make sure you understand every line. Use the Python documentation where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA: the TREC genomics benchmark\n",
    "\n",
    "For the following exercises, we will use a subset of the TREC genomics document collection and queries. \n",
    "It is stored in the folder `data01/` in the directory where you have been instructed to place the assignment notebooks (`/`).\n",
    "\n",
    "The collections contains:\n",
    "\n",
    "* `FIR-s05-medline.json` (the collection in Elasticsearch batch format - because of its size it cannot be indexed with a single curl command!)\n",
    "* `FIR-s05-training-queries-simple.txt` (test queries)\n",
    "* `FIR-s05-training-qrels.txt` (the \"relevance judgements\" for the test queries, i.e. the correct answers)\n",
    "\n",
    "> ___Note___ that these files contain a subset of the documents and queries of the TREC genomics track benchmark, to facilitate experimentations with less computation time needed.\n",
    "> The original files are also included in the `data01/` directory, withouth the `FIR-s05-` prefix (you may use them for the final project).\n",
    "\n",
    "To make things easy, the data is already provided in Elasticsearch' batch processing format. \n",
    "Inspect the collection file in the terminal:\n",
    "\n",
    "`head FIR-s05-medline.json`\n",
    "\n",
    "This shows the first 5 documents in the collection (in JSON format prepared for ElasticSearch, as you have seen in the tutorial)\n",
    "\n",
    "#### Baseline model and results\n",
    "We also provide the list of retrieved documents by a _baseline_ model, in the file `data01/baseline.run`. For each query, it contains the list of document IDs of the retrieved documents (to be compared with those in the qrels file). We use this file in the examples and evaluation exercises below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data01/FIR-s05-training-qrels.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_eval_files\u001b[39m(qrels_file, run_file):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_qrels_file(qrels_file), read_run_file(run_file)\n\u001b[0;32m---> 28\u001b[0m (all_relevant, all_retrieved) \u001b[38;5;241m=\u001b[39m \u001b[43mread_eval_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata01/FIR-s05-training-qrels.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata01/baseline.run\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mread_eval_files\u001b[0;34m(qrels_file, run_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_eval_files\u001b[39m(qrels_file, run_file):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_qrels_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqrels_file\u001b[49m\u001b[43m)\u001b[49m, read_run_file(run_file)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mread_qrels_file\u001b[0;34m(qrels_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_qrels_file\u001b[39m(qrels_file):  \u001b[38;5;66;03m# reads the content of he qrels file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     trec_relevant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()  \u001b[38;5;66;03m# query_id -> set([docid1, docid2, ...])\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqrels_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m qrels:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m qrels:\n\u001b[1;32m      5\u001b[0m             (qid, q0, doc_id, rel) \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data01/FIR-s05-training-qrels.txt'"
     ]
    }
   ],
   "source": [
    "def read_qrels_file(qrels_file):  # reads the content of he qrels file\n",
    "    trec_relevant = dict()  # query_id -> set([docid1, docid2, ...])\n",
    "    with open(qrels_file, 'r') as qrels:\n",
    "        for line in qrels:\n",
    "            (qid, q0, doc_id, rel) = line.strip().split()\n",
    "            if qid not in trec_relevant:\n",
    "                trec_relevant[qid] = set()\n",
    "            if (rel == \"1\"):\n",
    "                trec_relevant[qid].add(doc_id)\n",
    "    return trec_relevant\n",
    "\n",
    "def read_run_file(run_file):  \n",
    "    # read the content of the run file produced by our IR system \n",
    "    # (in the following exercises you will create your own run_files)\n",
    "    trec_retrieved = dict()  # query_id -> [docid1, docid2, ...]\n",
    "    with open(run_file, 'r') as run:\n",
    "        for line in run:\n",
    "            (qid, q0, doc_id, rank, score, tag) = line.strip().split()\n",
    "            if qid not in trec_retrieved:\n",
    "                trec_retrieved[qid] = []\n",
    "            trec_retrieved[qid].append(doc_id) \n",
    "    return trec_retrieved\n",
    "    \n",
    "\n",
    "def read_eval_files(qrels_file, run_file):\n",
    "    return read_qrels_file(qrels_file), read_run_file(run_file)\n",
    "\n",
    "(all_relevant, all_retrieved) = read_eval_files('data01/FIR-s05-training-qrels.txt', 'data01/baseline.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Number of queries_ and _number of retrieved documents per query_\n",
    " \n",
    "The following code counts the number of queries evaluated in the file `baseline.run` (provided in the `data01/` folder, containing the list of doc-ids retrieved using a baseline model) and prints it (use the result from the cell above). For each query, it also prints the number of documents that were retrieved for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of retrieved documents: %d' % len(all_retrieved))\n",
    "\n",
    "for qid in all_retrieved:\n",
    "    print ('Docs retrieved for query #{}: {}'.format(qid, str(len(all_retrieved[qid]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your own understanding, __inspect the structure and content of the `all_retrieved` and `all_relevant` data structures__ to understand them better. Use the `print()` function to see the content of the data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here the code to inspect the data structures\n",
    "rel_count_list = []\n",
    "retr_count_list = []\n",
    "for key in all_relevant:\n",
    "    print(len(all_relevant[key]), len(all_retrieved[key]))\n",
    "    rel_count_list.append(len(all_relevant[key]))\n",
    "    retr_count_list.append(len(all_retrieved[key]))\n",
    "print(sum(rel_count_list)/sum(retr_count_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.F: _mean average precision_\n",
    "__Using the `average_precision()` function you implemented above, write the code to compute the _Mean Average Precision_ for the `baseline.run` results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for key in all_relevant:\n",
    "        count += 1\n",
    "        total += average_precision(all_relevant[key], all_retrieved[key])\n",
    "    # END ANSWER\n",
    "    return total / count\n",
    "\n",
    "mapr = mean_average_precision(all_relevant, all_retrieved)\n",
    "print('Mean Average Precision (MAP): %1.5f\\n' % mapr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## TREC benchmark evaluation\n",
    "\n",
    "Below you find a function that take `all_relevant` and `all_retrieved` to compute the mean value of the `measure` over all queries. \n",
    "\n",
    "The function `mean_metric()`'s first function argument, `measure`, is a special argument: it is a function too! The `mean_metric` function sums the total score for the particular measure and divides it by the number of queries. It computes the average measures over all the query results.\n",
    "\n",
    "_This part will be reused later to compare the results of different models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_metric(measure, all_relevant, all_retrieved):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for qid in all_relevant:\n",
    "        relevant  = all_relevant[qid]\n",
    "        retrieved = all_retrieved.get(qid, [])\n",
    "        value = measure(relevant, retrieved)\n",
    "        total += value\n",
    "        count += 1\n",
    "    return \"mean \" + measure.__name__, total / count\n",
    "\n",
    "# Example of use of the mean_metric function: computing the average r_precision\n",
    "mean_metric(r_precision, all_relevant, all_retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC overview of the results of a run\n",
    "The following two functions use your implementation of the metrics to create an overview of the performance metrics on the TREC benchmark data. Give a look at the numbers and make your own interpretations of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trec_eval(qrels_file, run_file):\n",
    "\n",
    "    def precision_at_1(rel, ret): return precision_at_k(rel, ret, k=1)\n",
    "    def precision_at_5(rel, ret): return precision_at_k(rel, ret, k=5)\n",
    "    def precision_at_10(rel, ret): return precision_at_k(rel, ret, k=10)\n",
    "    def precision_at_50(rel, ret): return precision_at_k(rel, ret, k=50)\n",
    "    def precision_at_100(rel, ret): return precision_at_k(rel, ret, k=100)\n",
    "    def precision_at_recall_00(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.0)\n",
    "    def precision_at_recall_01(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.1)\n",
    "    def precision_at_recall_02(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.2)\n",
    "    def precision_at_recall_03(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.3)\n",
    "    def precision_at_recall_04(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.4)\n",
    "    def precision_at_recall_05(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.5)\n",
    "    def precision_at_recall_06(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.6)\n",
    "    def precision_at_recall_07(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.7)\n",
    "    def precision_at_recall_08(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.8)\n",
    "    def precision_at_recall_09(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.9)\n",
    "    def precision_at_recall_10(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=1.0)\n",
    "\n",
    "    (all_relevant, all_retrieved) = read_eval_files(qrels_file, run_file)\n",
    "    \n",
    "    unknown_qids = set(all_retrieved.keys()).difference(all_relevant.keys())\n",
    "    if len(unknown_qids) > 0:\n",
    "        raise ValueError(\"Unknown qids in run: {}\".format(sorted(list(unknown_qids))))\n",
    "\n",
    "    metrics = [success_at_1,\n",
    "               success_at_5,\n",
    "               success_at_10,\n",
    "               r_precision,\n",
    "               precision_at_1,\n",
    "               precision_at_5,\n",
    "               precision_at_10,\n",
    "               precision_at_50,\n",
    "               precision_at_100,\n",
    "               precision_at_recall_00,\n",
    "               precision_at_recall_01,\n",
    "               precision_at_recall_02,\n",
    "               precision_at_recall_03,\n",
    "               precision_at_recall_04,\n",
    "               precision_at_recall_05,\n",
    "               precision_at_recall_06,\n",
    "               precision_at_recall_07,\n",
    "               precision_at_recall_08,\n",
    "               precision_at_recall_09,\n",
    "               precision_at_recall_10,\n",
    "               average_precision]\n",
    "\n",
    "    return [mean_metric(metric, all_relevant, all_retrieved) for metric in metrics]\n",
    "\n",
    "\n",
    "def print_trec_eval(qrels_file, run_file):\n",
    "    results = trec_eval(qrels_file, run_file)\n",
    "    print(\"Results for {}\".format(run_file))\n",
    "    for (metric, score) in results:\n",
    "        print(\"{:<30} {:.4}\".format(metric, score))\n",
    "\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'data01/baseline.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 01.G: _Significance testing_\n",
    "\n",
    "Testing the statistical significance of differences of the results of different IR systems is important (see slides of lecture 01 and course book, Section 8.8). One of the basic tests one can perform is the two-tailed [sign test](https://en.wikipedia.org/wiki/Sign_test).\n",
    "\n",
    "Only for this exercise, we use the run files obtained by  [Hiemstra and Aly](https://djoerdhiemstra.com/wp-content/uploads/trec2014mirex-draft.pdf) for the TREC Web track 2014 benchmark (note these files are from a different benchmark from what we have been working with so far). The `utbase.run` file was generated using Language Modeling, while `utexact.run` was generated using an IR system based on mathing the exact query string, and ranking the documents by  the number of exact matches found. The exact run improves the _Precision at 5_ to 0.456 (compared to 0.440 for the baseline run).  \n",
    "\n",
    "__Implement the code to perform the _sign test_ of statistical significance.__\n",
    "> _Hint:_ for each sign, compute the number of queries that increase/descrease performance (called `better, worse` in the code below). How would you use these values to compute the _p_ value of the two-tailed sign test? Is the difference between _utbase_ and _utexact_ significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def sign_test_values(measure, qrels_file, run_file_1, run_file_2):\n",
    "    all_relevant = read_qrels_file(qrels_file)\n",
    "    all_retrieved_1 = read_run_file(run_file_1)\n",
    "    all_retrieved_2 = read_run_file(run_file_2)\n",
    "    better = 0\n",
    "    worse  = 0\n",
    "    # BEGIN ANSWER\n",
    "    for key in all_relevant:\n",
    "        #print(sorted(all_relevant[key]))\n",
    "        #print(all_retrieved_1[key][:5])\n",
    "        #print(all_retrieved_2[key][:5])\n",
    "        if precision_at_rank_5(all_relevant[key], all_retrieved_1[key]) > precision_at_rank_5(all_relevant[key], all_retrieved_2[key]):\n",
    "            worse += 1\n",
    "            #print(precision_at_rank_5(all_relevant[key], all_retrieved_1[key]), precision_at_rank_5(all_relevant[key], all_retrieved_2[key]))\n",
    "        elif precision_at_rank_5(all_relevant[key], all_retrieved_1[key]) < precision_at_rank_5(all_relevant[key], all_retrieved_2[key]):\n",
    "            better += 1\n",
    "            #print(\"!\", precision_at_rank_5(all_relevant[key], all_retrieved_1[key]), precision_at_rank_5(all_relevant[key], all_retrieved_2[key]))\n",
    "\n",
    "    # END ANSWER\n",
    "    return(better, worse)\n",
    "    \n",
    "def precision_at_rank_5(rel, ret):\n",
    "    return precision_at_k(rel, ret, k=5)\n",
    "\n",
    "vals = sign_test_values(precision_at_rank_5, 'data01/trec.qrels', 'data01/utbase.run', 'data01/utexact.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN ANSWER\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "\n",
    "binom_test(x=vals[0], n=vals[0]+vals[1], p=1/2, alternative='two-sided')\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "# Part 02 - Indexing and querying with ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Getting started with Elasticsearch\n",
    "\n",
    "The following parts of the assignment will be based on ElasticSearch. you are adviced to go through the \"Elasticsearch, [reference guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html)\", and work on the tutorials. You can skip the section on [Installation](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html), as we provide it already installed in the Virtual Machine.\n",
    "\n",
    "> If you want (disclaimer: we do __not__ give help with this!), you can \n",
    "> follow the [Installation](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html) to run Elasticsearch on your laptop without VM. Beware your system will likely be different from the \n",
    "> one of your colleagues and they might not be able to help you if \n",
    "> you have problems that are specific to your system, your operating\n",
    "> system, or your Elasticsearch version.\n",
    "\n",
    "### Starting/Stopping ElasticSearch\n",
    "To start ElasticSearch on the virtual machine, you can type `sudo service elasticsearch start` in a Terminal.\n",
    "To stop the ElasticSearch server, instead, you can type `sudo service elasticsearch stop`. Refer at the [the official guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html#deb-running-init), for more information.\n",
    "\n",
    "### The REST API\n",
    "\n",
    "Elasticsearch runs its own server that can be accessed by a regular web browser by opening this link: http://localhost:9200. \n",
    "\n",
    "Elasticsearch will respond with something like:\n",
    "\n",
    "    {\n",
    "        \"name\" : \"fir-machine\",\n",
    "        \"cluster_name\" : \"elasticsearch\",\n",
    "        \"cluster_uuid\" : \"w7SBVo1ESVivMApbLIqRvA\",\n",
    "        \"version\" : {\n",
    "            \"number\" : \"7.9.0\",\n",
    "            \"build_flavor\" : \"default\",\n",
    "            \"build_type\" : \"deb\",\n",
    "            \"build_hash\" : \"a479a2a7fce0389512d6a9361301708b92dff667\",\n",
    "            \"build_date\" : \"2020-08-11T21:36:48.204330Z\",\n",
    "            \"build_snapshot\" : false,\n",
    "            \"lucene_version\" : \"8.6.0\",\n",
    "            \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
    "            \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
    "        },\n",
    "        \"tagline\" : \"You Know, for Search\"\n",
    "    }\n",
    "\n",
    "\n",
    "If you see this, then your Elasticsearch node is up and running. The RESTful API uses simple text or JSON over HTTP. \n",
    "\n",
    "> REST, API, JSON, HTTP, that's a lot of abbreviations! It is good to\n",
    "> be familiar with the terminology. Let us explain: The Elasticsearch\n",
    "> response is not (only) intended for humans. It is supposed to be used \n",
    "> by applications that run on the client machines, and therefore the\n",
    "> interface is called an Application Programming Interface (API). The \n",
    "> API uses a format called JSON (JavaScript Object Notation), which \n",
    "> can be easily read by machines (and humans). The API sends its JSON\n",
    "> response using the same method as your web browser displays web\n",
    "> pages. This method is called HTTP (Hyper Text Transfer Protocol), \n",
    "> and it is the reason you can inspect the response in a normal web\n",
    "> browser. APIs that use HTTP are called RESTful interfaces. REST \n",
    "> stands for REpresentational State Transfer, arguably one of the\n",
    "> simplest ways to define an API.\n",
    "\n",
    "\n",
    "### Interacting with the ElasticSearch server\n",
    "\n",
    "You can interact with your Elasticsearch service in different ways. In this first part we explore Kibana, a dashboard for inspection of your indices. Later during the practical work we will use the Python Elasticsearch client or the DSL library. You can also start yourself with Python.\n",
    "\n",
    "#### Kibana\n",
    "Kibana provides a web interface to interact with your Elasticsearch service. It's available from http://localhost:5601. You can use Kibana to create interactive dashboards visualizing data in your Elasticsearch indices. It also provides a console to execute Elasticsearch commands. It's available from http://localhost:5601/app/kibana#/dev_tools\n",
    "\n",
    "To start Kibana on the virtual machine, you can type `sudo service kibana start` in a Terminal. \\\n",
    "To stop the Kibana server, instead, you can type `sudo service kibana stop`.\n",
    "\n",
    "Many examples from the Elasticsearch user guide can be directly executed in Kibana by clicking on the `CONSOLE` button.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing and queries (Exercises - Part 02)\n",
    "\n",
    "_You can work on this part after Lecture 01 already_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection indexing: useful code\n",
    "\n",
    "We provide some code to read the TREC collection documents and index them into the ElasticSearch engine.\n",
    "As we need to re-index the document collection when we use a different indexing configurations (called Mappings in ElasticSearch), we developed some functions to support a quick re-indexing in the following exercises.\n",
    "\n",
    "Below you find the Python code for bulk-indexing our (FIR)Medline collection. Execute the following cells to index the collection in an Elasticsearch index called `genomics'. Study the code carefully, as you will use the indexing functions later for the completion of the assignment.\n",
    "\n",
    "> The code uses additional helper functions \n",
    "> (`elasticsearch.helpers`) and a library for processing JSON.\n",
    "> The function `read_documents()` reads the bulk collection file: The \n",
    "> function is a [Python generator](https://wiki.python.org/moin/Generators) function. It generates an 'on-demand' list\n",
    "> by using the statement `yield` for every item of the list. It\n",
    "> is used in the helper function `elasticsearch.helpers.bulk()`.\n",
    "> The statement `raise` is Python's approach to throw exceptions: it exits the program with an error.\n",
    "> Note the (keyword) arguments to bulk:\n",
    "> `chunk_size` indicates the number of documents to be processed by\n",
    "> elasticsearch in one batch. \n",
    "> The request_timeout is set to 30 seconds because processing a single batch\n",
    "> of documents can take some time.\n",
    "\n",
    "> __Note:__ _when processing a bulk index, be sure to have few GigaBytes free on the hard drive of the VM. If you get a BulkIndexError with read-only/FORBIDDEN errors, you probably have too little hard drive space available for ElasticSearch to work properly._\n",
    "\n",
    "\n",
    "**_Note:_ indexing the (FIR)TREC genomics collection can take some time, be patient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import elasticsearch.helpers\n",
    "import json\n",
    "\n",
    "def read_documents(file_name):\n",
    "    \"\"\"\n",
    "    Returns a generator of documents to be indexed by elastic, read from file_name\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as documents:\n",
    "        for line in documents:\n",
    "            doc_line = json.loads(line)\n",
    "            if ('index' in doc_line):\n",
    "                id = doc_line['index']['_id']\n",
    "            elif ('PMID' in doc_line):\n",
    "                doc_line['_id'] = id\n",
    "                yield doc_line\n",
    "            else:\n",
    "                raise ValueError('Woops, error in index file')\n",
    "\n",
    "def create_index(es, index_name, body={}):\n",
    "    # delete index when it already exists\n",
    "    es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    # create the index \n",
    "    es.indices.create(index=index_name, body=body)\n",
    "                \n",
    "def index_documents(es, collection_file_name, index_name, body={}):\n",
    "    create_index(es, index_name, body)\n",
    "    # bulk index the documents from file_name\n",
    "    return elasticsearch.helpers.bulk(\n",
    "        es, \n",
    "        read_documents(collection_file_name),\n",
    "        index=index_name,\n",
    "        chunk_size=2000,\n",
    "        request_timeout=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the ElasticSearch server\n",
    "es = elasticsearch.Elasticsearch(host='localhost')  # in case you use Docker, the host is 'elasticsearch'\n",
    "\n",
    "# Index the collection into the index called 'genomics'\n",
    "body = {} # no indexing options (leave default)\n",
    "index_documents(es, 'data01/FIR-s05-medline.json', 'genomics-base', body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can change the name of the index, in case you want to have different indices of the same collection created with different indexing settings, and compare the performance on the test queries. \n",
    "\n",
    "> E.g. you create two indices 'genomics01' and 'genomics02': genomics01 uses the default options, while genomics02 uses custom tokenizers. You will then have two indices with different characteristics (and probably different performance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.A: index properties and querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Query the index called 'genomics-base' and determine how many documents are indexed.__\n",
    "\n",
    "You can use Kibana (suggested for the time being - you can use the command line in Kibana), the Python ElasticSearch library or DSL. Report the code you implemented and the resulting number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write the code here\n",
    "# BEGIN ANSWER\n",
    "es.count(index='genomics-base')\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. How many documents containing the term `molecule` are there in your index? (searching all fields of the documents).__\n",
    "\n",
    "You can use Kibana (suggested for the time being - you can use the command line in Kibana), the Python ElasticSearch library or DSL. Report the code you implemented and the resulting number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write the code that generates the answer here (you may also use Kibana)\n",
    "# BEGIN ANSWER\n",
    "def get_number_of_results(es, query):\n",
    "    search_results = es.search(index='genomics-base', body=query)\n",
    "\n",
    "    return search_results['hits']['total']['value']\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"molecule\",\n",
    "            \"fields\": [\"*\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "get_number_of_results(es, query)\n",
    "\n",
    "\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. How many documents containing the term `molecular` are there in your index? (searching all fields of the documents).__\n",
    "\n",
    "You can use Kibana (suggested for the time being - you can use the command line in Kibana), the Python ElasticSearch library or DSL. Report the code you implemented and the resulting number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write the code that generates the answer here (you may also use Kibana)\n",
    "# BEGIN ANSWER\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"molecular\",\n",
    "            \"fields\": [\"*\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "get_number_of_results(es, query)\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. How many documents containing the terms `cell` AND `blood` are there in your index? (searching all fields of the documents).__\n",
    "\n",
    "You can use Kibana (suggested for the time being - you can use the command line in Kibana), the Python ElasticSearch library or DSL. Report the code you implemented and the resulting number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write the code that generates the answer here (you may also use Kibana)\n",
    "# BEGIN ANSWER\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"query_string\": {\n",
    "                        \"query\": \"cell\",\n",
    "                        \"fields\": [\"*\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"query_string\": {\n",
    "                        \"query\": \"blood\",\n",
    "                        \"fields\": [\"*\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "get_number_of_results(es, query)\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "es = elasticsearch.Elasticsearch(host='localhost')  # in case you use Docker, the host is 'elasticsearch'\n",
    "\n",
    "# this is another solution (if query_string is used, be sure that AND is in the query, otherwise it will not search properly)\n",
    "term = 'blood AND cell'\n",
    "body = {\"track_total_hits\": True, \"query\": {\"query_string\": \n",
    "                                            {\"query\": term, \n",
    "                                             \"default_operator\":\"AND\", \n",
    "                                             \"auto_generate_synonyms_phrase_query\": True }}}\n",
    "result = es.search(index='genomics-base', body=body)\n",
    "print(\"Number of results: {}\".format(result['hits']['total']['value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.B: the Python ElasticSearch library\n",
    "\n",
    "#### Preparation\n",
    "The command line is fine for doing basic operations on your Elasticsearch indices, but as soon as things get more complex, you better use custom client programs.\n",
    "We will use the [Elasticsearch client library for Python](https://elasticsearch-py.readthedocs.io). This library will execute the HTTP requests that you have used before (with CURL or Kibana). The library is pre-installed on the VM.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "__Write the code that searches the index for _\"molecule\"_ using the [search()](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.Elasticsearch.search) function.__ Your code will take at minimum the following steps:\n",
    "\n",
    "1. import the python library `elasticsearch`.\n",
    "2. open a connection with the Elasticsearch host `'elasticsearch'` with `Elasticsearch()`.\n",
    "3. execute a search with `search()` using the index `genomics-base`, and a correct query body.\n",
    "4. print the JSON output of Elasticsearch \n",
    "\n",
    "How many hits are there in your index? Is the result the same as in Exercise 02.A?\n",
    "\n",
    "> Elasticsearch runs on localhost on your laptop, at port 9200 (so as http://localhost:9200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "import elasticsearch\n",
    "\n",
    "# your code below\n",
    "# BEGIN ANSWER\n",
    "es = elasticsearch.Elasticsearch(host='localhost')\n",
    "query = {\n",
    "    \"track_total_hits\": True,\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"molecule\",\n",
    "            \"fields\": [\"*\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "response = es.search(index='genomics-base', body=query)\n",
    "print(response)\n",
    "# END ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python client library returns Python objects, that use [dictionaries](https://docs.python.org/3.6/tutorial/datastructures.html#dictionaries) and [lists](https://docs.python.org/3.6/tutorial/introduction.html#lists).\n",
    "Use a [for loop](https://docs.python.org/3.6/tutorial/controlflow.html#for-statements) to inspect each hit, and print the retrieved document's titles one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "print(\"Number of results: {}\".format(response['hits']['total']['value']))\n",
    "# your code below\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['TI']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.C: _Search using the Elasticsearch DSL_\n",
    "\n",
    "You will notice that the native query format of Elasticsearch can be quite verbose.\n",
    "Elasticsearh provides the Python library `elasticsearch_dsl` to write more concise Elasticsearch queries. \n",
    "This is only to simplify the syntax: the library still issues Elasticsearch queries.\n",
    "\n",
    "For example, a simple `multi_match` query looks as follows:\n",
    "```python\n",
    "query = {\n",
    "   \"query\": {\n",
    "       \"multi_match\": {}\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "The same query can be created with the DSL as follows:\n",
    "```python\n",
    "query = Q(\"multi_match\")\n",
    "```\n",
    "\n",
    "Especially for more complicated boolean queries, to use the native query format can become complicated.\n",
    "Read more about the DSL [here](https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html)\n",
    "\n",
    "__1. Search for the query `molecule` and check whether you get the same number of results as for exercise 02.A(2).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# your code here\n",
    "# BEGIN ANSWER\n",
    "from elasticsearch_dsl import Q\n",
    "from elasticsearch_dsl import Search\n",
    "\n",
    "s = Search(using=es)\n",
    "\n",
    "q = Q(\"multi_match\", query='molecule', fields=['*'])\n",
    "\n",
    "response = s.query(q).execute()\n",
    "\n",
    "print(\"Number of results: {}\".format(response['hits']['total']['value']))\n",
    "\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Search for the documents that contain the words `cell` AND `blood`, using the DSL library. Check whether you get the same number of results as for exercise 02.A(4).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# your code here\n",
    "# BEGIN ANSWER\n",
    "\n",
    "q = Q(\"multi_match\", query='cell', fields=['*']) & Q(\"multi_match\", query='blood', fields=['*'])\n",
    "\n",
    "response = s.query(q).execute()\n",
    "\n",
    "print(\"Number of results: {}\".format(response['hits']['total']['value']))\n",
    "\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  Exercise 02.D: Making your own TREC run\n",
    "\n",
    "We will adopt a scientific approach to building search engines. That is, we are not only going to build a search engine and see that it works, but we are also going to _measure_ how well it works, by measuring the search engine's quality. We will adopt the method from the [Text Retrieval Conference](http://trec.nist.gov) (TREC). TREC provides researchers with test collections, that consists of 3 parts:\n",
    "\n",
    "1. the document collection (in our case a part of the MEDLINE database)\n",
    "2. the topics (which are natural language descriptions of what the user is searching for: you can think of the as the _queries_)\n",
    "3. the relevance judgments (for each topic, what documents are relevant)\n",
    "\n",
    "\n",
    "\n",
    "__Exercise: Complete the code of the Python function `make_trec_run()` that reads the topics [FIR-s05-training-queries-simple.txt](data01/FIR-s05-training-queries-simple.txt), and for each topic does a search using Elasticsearch.__ The program should output a file in the [TREC submission format](https://trec-core.github.io/2017/#submission-guidelines). We already provided the first  lines for this exercise, which include:\n",
    "\n",
    "1. Open the file `'run_file_name'`' for writing and call it `run_file`.\n",
    "2. Open the file `'topics_file_name'` for reading, call it `test_queries`.\n",
    "3. For each line in `test_queries`:\n",
    "4. Remove the newline using `strip()`, then split the string on the tab character (`'\\t'`). The first part of the line is now `qid` (the query identifier) and the last part is `query` (a textual description of the query).\n",
    "5. complete the Python program such that the correct TREC run file is written to `'run_file_name'`.\n",
    "\n",
    "> **Note**: Make sure you output the `PMID` (pubmed identifier) of the document `hit['_source']['PMID']`. Do **not** use the elasticsearch identifier `_id` because they do not match the document identifiers in the relevance judgements. They were randomly generated by Elasticsearch during indexing.\n",
    "\n",
    "\n",
    "__Make sure to search in the fiels `TI` and `AB`, which correspond to the title and abstract, respectivelt, of the scientific papers of the MEDLINE collection.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def make_trec_run(es, topics_file_name, run_file_name, index_name=\"genomics\", run_name=\"test\"):\n",
    "    with open(run_file_name, 'w') as run_file:\n",
    "        with open(topics_file_name, 'r') as test_queries:\n",
    "            for line in test_queries:\n",
    "                (qid, query) = line.strip().split('\\t')\n",
    "                # BEGIN ANSWER\n",
    "                s = Search(using=es, index=index_name)\n",
    "                s = s.extra(size=1000)\n",
    "\n",
    "                #q = Q(\"multi_match\", query=query.split()[0], fields=['TI', 'AB'])\n",
    "                \n",
    "                #for word in query.split():\n",
    "                    #q = q & Q(\"multi_match\", query=word, fields=['TI', 'AB'])\n",
    "                \n",
    "                q = Q(\"multi_match\", query=query, fields=['TI', 'AB'])\n",
    "\n",
    "                response = s.query(q).execute()\n",
    "                #print(\"Number of results: {}\".format(response['hits']['total']['value']))\n",
    "                #break\n",
    "                for i, hit in enumerate(response['hits']['hits']):\n",
    "                    run_file.write(f\"{qid} Q0 {hit['_source']['PMID']} {i} {hit['_score']} cj_search\\n\")\n",
    "\n",
    "                # END ANSWER\n",
    "                \n",
    "# connect to ES server             \n",
    "es = elasticsearch.Elasticsearch('localhost')\n",
    "# Write the results of the queries contained in the topic file `'data/training-queries-simple.txt'` \n",
    "# to the run file `'baseline.run'`, and name this test as `test01`\n",
    "make_trec_run(es, 'data01/FIR-s05-training-queries-simple.txt', 'baseline.run', \"genomics-base\", run_name='test01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prints out (it is a shell command) the content of the file baseline.run \n",
    "!cat baseline.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: Write a line to `run_file` using `run_file.write(line)`. \n",
    "> The newline character is: `'\\n'`. Before writing a number to\n",
    "> the file, cast it to a string using `str()`.\n",
    ">\n",
    "> The TREC Submission guidelines allow you to submit up to 1000\n",
    "> documents per topic. Keep this in mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 03: Index improvements: Tokenization\n",
    "<span style=\"background:red; color: white;\">__You are advised to work on this part after Lecture 02 (Conceptual Indexing)__</span>\n",
    "\n",
    "\n",
    "\n",
    "## Background\n",
    "The following part of the assignment requires some self-study of the ElasticSearch tools to support the improvemnet of the indexing. Please read the:\n",
    "* [Index Settings and Mappings](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/indices-create-index.html).\n",
    "* Elasticsearch [Analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/analysis.html) contain many options for improving your search engine.\n",
    "\n",
    "> You are suggested to use the [Python Elasticsearch Client](https://elasticsearch-py.readthedocs.io) library documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 03.A: _chat language analyzer_\n",
    "\n",
    "Read the documentation for [Custom Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/analysis-custom-analyzer.html). \n",
    "Make a custom analyzer for _English chat language_. The analyzer should do the following:\n",
    "* change common abbreviations to the full forms: \n",
    "  * _b4_ to _before_, \n",
    "  * _abt_ to _about_, \n",
    "  * _chk_ to _check_, \n",
    "  * _dm_ to _direct message_,\n",
    "  * _f2f_ to _face-to-face_\n",
    "* use the _standard_ tokenizer;\n",
    "* put everything to lower-case;\n",
    "* filter English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "tweet_analyzer = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"tweet_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\", \"b4\", \"abt\", \"chk\", \"dm\", \"f2f\", \"english_stop\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"b4\": {\n",
    "                    \"type\": \"pattern_replace\",\n",
    "                    \"pattern\": \"b4\",\n",
    "                    \"replacement\": \"before\"\n",
    "                },\n",
    "                \"abt\": {\n",
    "                    \"type\": \"pattern_replace\",\n",
    "                    \"pattern\": \"abt\",\n",
    "                    \"replacement\": \"about\"\n",
    "                },\n",
    "                \"chk\": {\n",
    "                    \"type\": \"pattern_replace\",\n",
    "                    \"pattern\": \"chk\",\n",
    "                    \"replacement\": \"check\"\n",
    "                },\n",
    "                \"dm\": {\n",
    "                    \"type\": \"pattern_replace\",\n",
    "                    \"pattern\": \"dm\",\n",
    "                    \"replacement\": \"direct message\"\n",
    "                },\n",
    "                \"f2f\": {\n",
    "                    \"type\": \"pattern_replace\",\n",
    "                    \"pattern\": \"f2f\",\n",
    "                    \"replacement\": \"face-to-face\"\n",
    "                },\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# create the index, but don't index any documents:\n",
    "create_index(es, 'genomics', body=tweet_analyzer)\n",
    "body = { \"field\": \"all\", \"text\": \"done it b4! what abt dm me?\", \"analyzer\": \"tweet_analyzer\"}\n",
    "tokens = es.indices.analyze(index='genomics', body=body)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 03.B: Stemmers\n",
    "\n",
    "Referring at Exercise 02.A, we have seen that queries like `molecule` and `molecular` retrieve different sets of documents. Lemmatizer and stemmers can help the indexing and search of 'similar' terms, and retrieve more consistent sets of documents.\n",
    "\n",
    "__Use the ElasticSearch [Stemming](https://www.elastic.co/guide/en/elasticsearch/reference/current/stemming.html) to index the document collection. Then retrieve documents with the queries `molecule` and `molecular` and comment on the eventual differences with the previous query results.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "body = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"stem_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"stemmer\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"name\": \"english\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#body = {\n",
    "  # BEGIN ANSWER\n",
    "  # END ANSWER\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the ElasticSearch server\n",
    "es = elasticsearch.Elasticsearch(host='localhost')  # in case you use Docker, the host is 'elasticsearch'\n",
    "\n",
    "# Index the collection into the index called 'genomics-stem'\n",
    "index_documents(es, 'data01/FIR-s05-medline.json', 'genomics-stem', body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Retrieve documents with the queries `molecule` and `molecular` and comment on the eventual differences with the previous query results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "# BEGIN ANSWER\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"molecule\",\n",
    "            \"fields\": [\"*\"],\n",
    "            \"analyzer\": \"stem_analyzer\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_results = es.search(index='genomics-stem', body=query)\n",
    "print(search_results['hits']['total']['value'])\n",
    "print(search_results)\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"molecules\",\n",
    "            \"fields\": [\"*\"],\n",
    "            \"analyzer\": \"stem_analyzer\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "search_results = es.search(index='genomics-stem', body=query)\n",
    "print(search_results['hits']['total']['value'])\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"molecular\",\n",
    "            \"fields\": [\"*\"],\n",
    "            \"analyzer\": \"stem_analyzer\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_results = es.search(index='genomics-stem', body=query)\n",
    "print(search_results['hits']['total']['value'])\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment here about the eventual different results you get \n",
    "# -> words that are stemmed, like molecule and molecules, should improve the retrieval results (in terms of amount of retrieved dos)\n",
    "# -> words that are not stemmed, like 'molecular' should not see much differernt results\n",
    "\n",
    "body = { \"field\": \"all\", \"text\": \"molecule it b4! what abt dm me?\", \"analyzer\": \"stem_analyzer\"}\n",
    "tokens = es.indices.analyze(index='genomics-stem', body=body)\n",
    "pprint(tokens)\n",
    "\n",
    "\n",
    "body = { \"field\": \"all\", \"text\": \"molecular it b4! what abt dm me?\", \"analyzer\": \"stem_analyzer\"}\n",
    "tokens = es.indices.analyze(index='genomics-stem', body=body)\n",
    "pprint(tokens)\n",
    "\n",
    "## As we can see molecule turns into molecul after stemming, and there is only one \n",
    "\n",
    "# the point of the exercise is not to have the same results with molecule and molecular, but \n",
    "# what the stemmer does, and reason after that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 04: Search models \n",
    "\n",
    "\n",
    "<span style=\"background:red; color: white;\">__You are advised to work on this part after Lecture 03__</span>\n",
    "\n",
    "\n",
    "### Background\n",
    "The way documents are indexed influences the performance of the IR systems. \n",
    "Elasticsearch [Mappings](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/mapping.html) define how a document, and its properties (fields) are stored and indexed, but also provides tools to implement and execute different document similarity measures (i.e. search models).  When using a different configuration of an ElasticSearch Mapping, the document collection needs to be re-indexed (or a new index need to be created - use the functions we provided above to do that).\n",
    "\n",
    "> See again: [Index Settings and Mappings](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/indices-create-index.html).\n",
    "> _Note: the default model (similarity) in ElasticSearch is BM25. Different models need to be specified (see example)._\n",
    "\n",
    "For instance, we can add a new field `\"title-abstract\"` that uses the  [similarity measure](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/similarity.html) _Boolean_, and let it serve as an index for the fields `\"TI\"` and `\"AB\"` (title and abstract):\n",
    "\n",
    "> Plase note that if you want to use the `boolean` similarity for the single fields, you need to specify it for each field. Otherwise, the default BM25 will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean = {\n",
    "  \"settings\" : {\n",
    "    # a single shard, so we do not suffer from approximate document frequencies\n",
    "    \"number_of_shards\" : 1\n",
    "  },\n",
    "  \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"AB\": {\n",
    "          \"type\": \"text\",\n",
    "          \"copy_to\": \"title-abstract\",\n",
    "          \"similarity\": \"boolean\"\n",
    "        },\n",
    "        \"TI\": {\n",
    "          \"type\": \"text\",\n",
    "          \"copy_to\": \"title-abstract\",\n",
    "          \"similarity\": \"boolean\"\n",
    "        },\n",
    "        \"title-abstract\": {  # compound field\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"boolean\"\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "es = elasticsearch.Elasticsearch('localhost')\n",
    "index_documents(es, 'data01/FIR-s05-medline.json', 'genomics-bool', body=boolean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most changes to the mappings cannot be done on an existing index. Some (for instance\n",
    "> similarity measures) can be changed if the index is first closed. Nevertheless, we \n",
    "> will in this notebook _re-index_ the collection for every change to the mappings\n",
    "> using the function `index_documents()` that we defined above. Mappings (and settings)\n",
    "> can be passed to the function using the `body` parameter.\n",
    "\n",
    "<span style=\"background:#444; color: white;\">__We suggest you to create different indices using different models of search (according to the available disk space on your VM). This will avoid that changes are not correctly applied, and you won't see the expected results.__</span>\n",
    "\n",
    "<span style=\"background:#444; color: white;\">E.g. for the 'boolean' model, we created the 'genomics-bool' index.</span>\n",
    "\n",
    "Let's have a look at the mappings and settings for our index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.get(index='genomics-bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search our new field `\"title-abstract\"` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"molecule\"\n",
    "search_type = \"dfs_query_then_fetch\" # this will use exact document frequencies even for multiple shards\n",
    "body = {\n",
    "  \"query\": {\n",
    "    \"match\" : { \"title-abstract\" : query }\n",
    "  },\n",
    "  \"size\": 10\n",
    "}\n",
    "es.search(index=\"genomics-bool\", search_type=search_type, body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 04.A: _new run and evaluation_\n",
    "Create a new run file (e.g. `boolean.run`), compute the retrieval performance with the function `print_trec_eval()` and compare the results with the baseline run file `baseline.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "# write your code here\n",
    "# BEGIN ANSWER\n",
    "def make_trec_run2(es, topics_file_name, run_file_name, index_name=\"genomics\", run_name=\"test\", field=\"title-abstract\"):\n",
    "    with open(run_file_name, 'w') as run_file:\n",
    "        with open(topics_file_name, 'r') as test_queries:\n",
    "            for line in test_queries:\n",
    "                (qid, query) = line.strip().split('\\t')\n",
    "                # BEGIN ANSWER\n",
    "                search_type = \"dfs_query_then_fetch\" # this will use exact document frequencies even for multiple shards\n",
    "                body = {\n",
    "                  \"query\": {\n",
    "                    \"match\" : { field : query }\n",
    "                  },\n",
    "                  \"size\": 1000\n",
    "                }\n",
    "                response = es.search(index=index_name, search_type=search_type, body=body)\n",
    "                for i, hit in enumerate(response['hits']['hits']):\n",
    "                    run_file.write(f\"{qid} {run_name} {hit['_source']['PMID']} {i} {hit['_score']} cj_search2\\n\")\n",
    "\n",
    "                # END ANSWER\n",
    "                \n",
    "# connect to ES server             \n",
    "es = elasticsearch.Elasticsearch('localhost', timeout=30)\n",
    "# Write the results of the queries contained in the topic file `'data/training-queries-simple.txt'` \n",
    "# to the run file `'baseline.run'`, and name this test as `test01`\n",
    "make_trec_run2(es, 'data01/FIR-s05-training-queries-simple.txt', 'boolean.run', \"genomics-bool\", run_name='test01')\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'baseline.run')\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'boolean.run')\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 04.B: _Language models_\n",
    "\n",
    "Custom similarities can be configured by tuning the parameters of the built-in similarities. Read more about these (expert) options in the [similarity module](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/index-modules-similarity.html).\n",
    "\n",
    "> Tip: the example similarity settings have to be used in a `\"settings\"` object.\n",
    "> Check your settings and mappings with: `es.indices.get(index='NAME-OF-INDEX')`.\n",
    "\n",
    "__1. Make a run that uses Language Models with [Jelinek-Mercer smoothing](http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html) (linear interpolation smoothing) on the field `\"all\"` that indexes the fields `\"TI\"` and `\"AB\"`. Use the parameter `lambda=0.2`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "lmjelinekmercer = {\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"similarity\": {\n",
    "        \"ljm\": {\n",
    "          \"type\": \"LMJelinekMercer\",\n",
    "          \"lambda\": 0.2\n",
    "        }\n",
    "      },\n",
    "    \"number_of_shards\" : 1\n",
    "    }\n",
    "  },\n",
    "    \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"AB\": {\n",
    "          \"type\": \"text\",\n",
    "          \"copy_to\": \"all\",\n",
    "          \"similarity\": \"ljm\"\n",
    "        },\n",
    "        \"TI\": {\n",
    "          \"type\": \"text\",\n",
    "          \"copy_to\": \"all\",\n",
    "          \"similarity\": \"ljm\"\n",
    "        },\n",
    "        \"all\": {  # compound field\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"ljm\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_documents(es, 'data01/FIR-s05-medline.json', 'genomics-jm', body=lmjelinekmercer)\n",
    "pprint(es.indices.get(index='genomics-jm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_trec_run2(es, 'data01/FIR-s05-training-queries-simple.txt', 'lmjelinekmercer.run', 'genomics-jm', run_name=\"testtest\", field=\"all\")\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'lmjelinekmercer.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Make a run that uses Language Models with [Dirichelet smoothing](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html) to index the fields `\"TI\"` and `\"AB\"`. Use the parameter `mu=2000`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "dirichlet = {\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"similarity\": {\n",
    "        \"ld\": {\n",
    "          \"type\": \"LMDirichlet\",\n",
    "          \"mu\": 2000\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "      \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"AB\": {\n",
    "          \"type\": \"text\",\n",
    "          \"copy_to\": \"all2\",\n",
    "          \"similarity\": \"ld\"\n",
    "        },\n",
    "        \"TI\": {\n",
    "          \"type\": \"text\",\n",
    "          \"copy_to\": \"all2\",\n",
    "          \"similarity\": \"ld\"\n",
    "        },\n",
    "        \"all2\": {  # compound field\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"ld\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_documents(es, 'data01/FIR-s05-medline.json', 'genomics-dirichlet', body=dirichlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_trec_run2(es, 'data01/FIR-s05-training-queries-simple.txt', 'dirichlet.run', 'genomics-dirichlet', \"testtesttest\", \"all2\")\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'dirichlet.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 04.C: _Model comparison_\n",
    "\n",
    "\n",
    "__1. Compute the performance results of the `lmjelinekmercer.run` and `dirichelet.run`. Compare them with those of the `baseline.run` and `boolean.run`. Evaluate the runs using the `print_trec_eval` function. Performing statistical tests may help strengthen your claims.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'baseline.run')\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'boolean.run')\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'dirichlet.run')\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'lmjelinekmercer.run')\n",
    "# your comments here\n",
    "# BEGIN ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top20 retrieved documents baseline.run')\n",
    "! head -10 baseline.run\n",
    "\n",
    "print('\\nTop20 retrieved documents boolean.run')\n",
    "! head -10 boolean.run\n",
    "\n",
    "print('\\nTop20 retrieved documents lmjelinekmercer.run')\n",
    "! head -10 lmjelinekmercer.run\n",
    "\n",
    "print('\\nTop20 retrieved documents dirichlet.run')\n",
    "! head -10 dirichlet.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__2. Provide below your comments and interpretations of the results. Why, in your opinion, one model of search is better than the others?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer as a comment:\n",
    "# Using the Jelinek-Mercer smoothing yields by far the best mean average precision, beating the others by about 0.04.\n",
    "# However using the boolean similarity seems to result in better first results.\n",
    "\n",
    "# There are documents that a boolean model would disregard as those don't contain all query tokens, however a smoothing\n",
    "# algorithm considers those too. This might be why the mean average precision is higher.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: _ElasticSearch Analyzers for tokenization_\n",
    "\n",
    "The amount and quality of the tokens used to construct the inverted index are of great importance. In ElasticSearch, mappings and settings also allow specifying what [Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html) is used to tokenize your documents and queries. In the mappings below, use the _Dutch_ analyzer for the field `\"all\"`):\n",
    "\n",
    "> Usually, the same analyzer should be applied to documents and queries, but \n",
    "> Elasticsearch allows you to specify a `\"search_analyzer\"` that is used on \n",
    "> your queries (which we do not need to use in the assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_test = {\n",
    "  \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"all\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"dutch\"\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "# create the index, but don't index any documents:\n",
    "create_index(es, 'test-tokens', body=analyzer_test)\n",
    "\n",
    "analyzer_letter = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"analyzer_letter\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"letter\"}}}},\n",
    "  \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"all\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"dutch\"\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "# create the index, but don't index any documents:\n",
    "create_index(es, 'test-tokens', body=analyzer_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzer defined for the `\"all\"` field can be tested [as follows](https://elasticsearch-py.readthedocs.io/en/master/api.html#indices). Translated to English the text says: _\"This is a Dutch sentence\"_. \n",
    "\n",
    "> The following script identifies the tokens (based on the use of the dutch tokenizer): try with different tokenizers and different sentences to see how the tokens are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint # pretty print\n",
    "\n",
    "body = { \"field\": \"all\", \"text\": \"dit zijn nederlandse zinnen\"}\n",
    "tokens = es.indices.analyze(index='test-tokens', body=body, analyzer=\"analyzer_letter\")\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS PART: _Implement your own similarity measure_ \n",
    "\n",
    "We have only seen the results of using the analyzer to queries. The analyzer results from the _documents_ are available using the `termvectors()` function, as follows for document `id=3`: (Additionally, we can get overall field statistics, such as the number of documents)\n",
    "\n",
    "> First, index the collection again. While waiting, have a coffee or tea :) \n",
    "\n",
    "> `id=3` refers to the internal document identifiers, so not to the Pubmed identifier.\n",
    "\n",
    "_The bonus exercise is not mandatory. It can compensate for lower grades of other exercises._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "es = elasticsearch.Elasticsearch(host='localhost')\n",
    "\n",
    "# index_documents(es, 'data/FIR-05-medline.json', 'genomics-base')\n",
    "\n",
    "es.termvectors(index=\"genomics-base\", id=\"3\", fields=\"TI\", \n",
    "               term_statistics=True, field_statistics=True, offsets=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the BM25 similarity\n",
    "\n",
    "Complete the function `bm25_similarity()` below by implementing the BM25 similarity as described by in Section 11.4.3 of [Manning, Raghavan and Schuetze, Chapter 11](https://nlp.stanford.edu/IR-book/pdf/11prob.pdf). Are you able to replicate the score of ElasitcSearch (9.55)? If not, are you using a different variant of the BM25 model? Provide your comments in plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "import math\n",
    "\n",
    "# math.log(x) computes the logarithm of x\n",
    "\n",
    "def bm25_similarity (query, doc_id):\n",
    "\n",
    "    # Get the query tokens (see above)\n",
    "    query_tokens = es.indices.analyze(index='genomics-base', body={\"field\":\"TI\", \"text\": query})\n",
    "    tokens = query_tokens['tokens']\n",
    "\n",
    "    # Get the term vector for doc_id and the field statistics\n",
    "    term_vector = es.termvectors(index=\"genomics-base\", id=doc_id, fields=\"TI\", \n",
    "                  term_statistics=True, field_statistics=True, offsets=False)\n",
    "    vector = term_vector['term_vectors']['TI']['terms']\n",
    "    f_stats = term_vector['term_vectors']['TI']['field_statistics']\n",
    "\n",
    "    # The answer should sum over 'tokens', check if the tokens exists in the 'vector',\n",
    "    # and if so, add the appropriate value to 'similarity'.\n",
    "    # Tip: add print statements to your code to see what each variable contains.\n",
    "    \n",
    "    similarity = 0\n",
    "\n",
    "    # BEGIN ANSWER\n",
    "    # END ANSWER\n",
    "    return similarity\n",
    "\n",
    "bm25_similarity(\"structure refinement\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eventual comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below the 'reference score' computed by ElasticSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"query\": {\n",
    "    \"match\" : { \"TI\" : \"structure refinement\" }\n",
    "  }\n",
    "}\n",
    "explain = es.explain(index=\"genomics-base\", id=\"3\", body=body)\n",
    "print (explain['explanation']['value'])  # BM25 score computed by ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
